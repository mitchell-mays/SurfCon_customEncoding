{"cells":[{"cell_type":"markdown","metadata":{"id":"S31t7cfcW-1f"},"source":["# CS598 Deep Learning for Healthcare\n","### Mitch & Sathish\n","\n","## Reproducability Summary\n","This project recreates the work done in the paper -> [SurfCon: Synonym Discovery on Privacy-Aware Clinical Data](https://arxiv.org/pdf/1906.09285.pdf). The goal of the paper is to utilize aggregated Co-frequency data of medical terms from Clinical Notes as an indicator of relationships between these terms. For this paper, the authors demonstrated the ability to identify  synonymous terms (indicated by alignment under the same UMLS Concept). Our team was able to reproduce positive results akin to to the results seen by the authors of the Surfcon Paper. We reproduced two of the expirements done by the authors, with a slight variation of datasets. The character based pre-trained embeddings utilized by the original authors were unavailable, and therefore we identified and utilized a different subword embeddings instead. \n","\n","## Description & Context\n","\n","\n","While the Authors did make [code](https://github.com/zhenwang9102/SurfCon) available for the models and training, modifications were required for the following key elements:\n","- Data loading\n","- Data pre-processing\n","    - Transform data structure for inputs\n","    - Implement PPMI algorithm to convert frequency to PPMI\n","    - Implement subsampling algorithm\n","    - Map & Create synonym graph for labels\n","- Update outdated packages\n","\n","Additionally, as the character based (surface form) pre-trained embedding was not available, we worked with the authors to find a suitable alternative (see below), and wrote code to pre-process this data as the structure of the datasets differed.\n","\n","Therefore, the referenced code includes a combination of the authors original code, modifications made by our team, and new code generated by our team."]},{"cell_type":"markdown","metadata":{"id":"WK7sCE1yW-1m"},"source":["## Data Loading\n","\n","The main dataset utilized for this research is a co-frequency graph built from clinical narrative data in this paper: [Building the graph of medicine from millions of clinical narratives](https://datadryad.org/resource/doi:10.5061/dryad.jp917). The graph data is available in the paper link above.\n","\n","The pretrained embeddings utilized in the paper can be downloaded here: \n","- Word Embeddings -> [GloVe](http://nlp.stanford.edu/data/glove.6B.zip)\n","- Node Embeddings -> [Node Embeddings](https://drive.google.com/file/d/1nKXDppoSsT6uHCl0yG_zlrC4QFyCyu41/view)\n","\n","The pretrained embeddings that our team used to replace the CharNGram in the paper:\n","- Subword Embeddings -> Fastext [Fastext pretrained subword embeddings](https://fasttext.cc/docs/en/english-vectors.html)"]},{"cell_type":"markdown","metadata":{"id":"X08qwuO1W-1o"},"source":["1. Download co-frequency graph, unzip and store in the `mappings` folder\n","2. Download embeddings, unzip, and store in the `embeddings` folder\n","3. To replicate the medical terms that the original authors used, download [these](https://drive.google.com/file/d/1RN0x45dnMAkRKQWAwIqoz2qNL_3hfsQv/view) `pkl` files and store them as follows:\n","    - `all_iv_terms_perBin_1.pkl` in the `sym_data` folder\n","    - `term_string_mapping.pkl` in the `mappings` folder\n","\n","\n","Alternatively, if unable to replicate the data folder structure as needed feel free to pull down the [data folder](https://drive.google.com/drive/folders/1WWg-rEqJl1A-5IM3nQHT93hIfpXaSG4-?usp=sharing) from our development which includes all of the datasets in the correct locations "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":394,"status":"ok","timestamp":1683382857879,"user":{"displayName":"Sathish Rama","userId":"12952249707319218415"},"user_tz":240},"id":"RkIjfuZRXuxH","outputId":"224b304b-ee62-40e4-c928-f5c6d7c19a27"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/cs598_dlh_project\n"]}],"source":["cd /content/drive/MyDrive/cs598_dlh_project/"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12277,"status":"ok","timestamp":1683382871863,"user":{"displayName":"Sathish Rama","userId":"12952249707319218415"},"user_tz":240},"id":"FBad7Gd4W-1p","outputId":"9684fc9d-365e-40b5-cc59-d34251c53973"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting numpy==1.24.2\n","  Downloading numpy-1.24.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: networkx==3.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (3.1)\n","Requirement already satisfied: scipy==1.10.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (1.10.1)\n","Requirement already satisfied: scikit-learn==1.2.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (1.2.2)\n","Collecting tqdm==4.28.1\n","  Downloading tqdm-4.28.1-py2.py3-none-any.whl (45 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torch==2.0.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (2.0.0+cu118)\n","Collecting jellyfish==0.7.1\n","  Downloading jellyfish-0.7.1.tar.gz (131 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.1/131.1 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting Distance==0.1.3\n","  Downloading Distance-0.1.3.tar.gz (180 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.3/180.3 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting pytorch-nlp==0.5.0\n","  Downloading pytorch_nlp-0.5.0-py3-none-any.whl (90 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.1/90.1 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.2.2->-r requirements.txt (line 4)) (3.1.0)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.2.2->-r requirements.txt (line 4)) (1.2.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->-r requirements.txt (line 6)) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->-r requirements.txt (line 6)) (2.0.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->-r requirements.txt (line 6)) (3.12.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->-r requirements.txt (line 6)) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->-r requirements.txt (line 6)) (1.11.1)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.0->-r requirements.txt (line 6)) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.0->-r requirements.txt (line 6)) (16.0.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.0->-r requirements.txt (line 6)) (2.1.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.0->-r requirements.txt (line 6)) (1.3.0)\n","Building wheels for collected packages: jellyfish, Distance\n","  Building wheel for jellyfish (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for jellyfish: filename=jellyfish-0.7.1-cp310-cp310-linux_x86_64.whl size=82826 sha256=b7bbe38279770d531dd6badbc58d299a224083d1f5a0aba3bf7fc22fe45a9b5e\n","  Stored in directory: /root/.cache/pip/wheels/dc/26/dd/9048c0b5f90433d6a48bf2cff931e0e76dccbc0d9d9962dba6\n","  Building wheel for Distance (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for Distance: filename=Distance-0.1.3-py3-none-any.whl size=16275 sha256=e038003d9112e4d9f18e576b06ed0f5e2a50d993ede2392ef3080b5d7877ad78\n","  Stored in directory: /root/.cache/pip/wheels/e8/bb/de/f71bf63559ea9a921059a5405806f7ff6ed612a9231c4a9309\n","Successfully built jellyfish Distance\n","Installing collected packages: Distance, tqdm, numpy, jellyfish, pytorch-nlp\n","  Attempting uninstall: tqdm\n","    Found existing installation: tqdm 4.65.0\n","    Uninstalling tqdm-4.65.0:\n","      Successfully uninstalled tqdm-4.65.0\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.22.4\n","    Uninstalling numpy-1.22.4:\n","      Successfully uninstalled numpy-1.22.4\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 1.24.2 which is incompatible.\n","spacy 3.5.2 requires tqdm<5.0.0,>=4.38.0, but you have tqdm 4.28.1 which is incompatible.\n","prophet 1.1.2 requires tqdm>=4.36.1, but you have tqdm 4.28.1 which is incompatible.\n","panel 0.14.4 requires tqdm>=4.48.0, but you have tqdm 4.28.1 which is incompatible.\n","numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.24.2 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed Distance-0.1.3 jellyfish-0.7.1 numpy-1.24.2 pytorch-nlp-0.5.0 tqdm-4.28.1\n"]}],"source":["# Install Python packages\n","!pip install -r requirements.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1683382875388,"user":{"displayName":"Sathish Rama","userId":"12952249707319218415"},"user_tz":240},"id":"qVa9QVq4YIuH","outputId":"90a45ac9-24c6-47ea-c0f3-f300bf91e758"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/cs598_dlh_project/src\n"]}],"source":["cd src"]},{"cell_type":"markdown","metadata":{"id":"ssVSi8ZCW-1u"},"source":["## Data Pre-processing"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":35857,"status":"ok","timestamp":1683382913273,"user":{"displayName":"Sathish Rama","userId":"12952249707319218415"},"user_tz":240},"id":"iRDcnAwRW-1v","outputId":"f1dbfe4f-7f5a-4987-ac3c-1321ac399cb1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Getting subsampled terms\n","Terms before subsampling: 56594\n","Terms after subsampling: 55985\n","Finished retrieving subsampled terms\n","        \n","Subsampling and generating PPMI cofrequency graph\n","Graph length before subsampling: 61824936\n","Graph length after subsampling: 46388854\n","      \n","Final Graph length filtered to Postive PPMI: 419062\n","PPMI Max: 9.072313000642843, Mean: 1.249204815728219\n","      \n","Skipping write of co-frequency dataset\n","File written\n","Building train/test synonym graphs\n","Skipping storage of graphs.\n"]}],"source":["import pandas as pd\n","import numpy as np\n","import pickle\n","import networkx as nx\n","\n","examplePPMIRow = None\n","exampleSynonym = None\n","\n","# To re-write the processed datasets, set this value to True\n","writeDatasets = False\n","\n","# ======================================\n","# Convert Co-frequency count graph to PPMI\n","# ======================================\n","\n","def cofreq_to_ppmi(cofreq_graph, singleton_freqs):\n","    # Add ppmi column to graph after calculating PPMI from term frequency\n","    wordCount = len(singleton_freqs.index)    \n","    singleton_freqs = singleton_freqs.rename(columns={\"node\": \"node1\", \"count\": \"count_1\"}) \n","    cofreqIncludeTermCounts = cofreq_graph.merge(singleton_freqs, how='inner', on=[\"node1\"])\n","    \n","    singleton_freqs = singleton_freqs.rename(columns={\"node1\": \"node2\", \"count_1\": \"count_2\"}) \n","    cofreqIncludeTermCounts = cofreqIncludeTermCounts.merge(singleton_freqs, how='inner', on=[\"node2\"])     \n","\n","    cofreqIncludeTermCounts[\"ppmi\"] = np.maximum(0, np.log2((cofreqIncludeTermCounts[\"count\"] / wordCount) / ((cofreqIncludeTermCounts[\"count_1\"] / wordCount) * (cofreqIncludeTermCounts[\"count_2\"] / wordCount))))\n","\n","    cofreq_graph = cofreqIncludeTermCounts[[\"node1\", \"node2\", \"count\", \"ppmi\"]]\n","\n","    return cofreq_graph\n","\n","\n","\n","def generateDatasetPkl():\n","    global examplePPMIRow\n","\n","    print(\"Getting subsampled terms\")\n","    singleton_freqs = subsampleTerms()\n","    print(\"Finished retrieving subsampled terms\")    \n","    print('        ')\n","\n","    print('Subsampling and generating PPMI cofrequency graph')\n","    cofreq_graph = pd.read_csv('../data/mappings/1_Cofrequencies/cofreqs_terms_perBin_1d.txt', delim_whitespace=True)\n","    cofreq_graph.columns = [\"node1\", \"node2\", \"count\"]\n","    \n","    print('Graph length before subsampling: {0}'.format(len(cofreq_graph.index)))\n","    cofreq_graph = cofreq_graph[cofreq_graph[\"node1\"] != cofreq_graph[\"node2\"]]\n","    cofreq_graph = cofreq_to_ppmi(cofreq_graph, singleton_freqs)\n","    print('Graph length after subsampling: {0}'.format(len(cofreq_graph.index)))\n","\n","    # Remove all co-frequency edges where PPMI is 0\n","    cofreq_graph = cofreq_graph[cofreq_graph[\"ppmi\"] > 0]\n","    print('      ')\n","    print('Final Graph length filtered to Postive PPMI: {0}'.format(len(cofreq_graph.index)))\n","    print('PPMI Max: {0}, Mean: {1}'.format(np.max(cofreq_graph[\"ppmi\"]), np.mean(cofreq_graph[\"ppmi\"])))\n","    print('      ')\n","\n","    datasetDict = {}    \n","    \n","    node1 = cofreq_graph['node1'].to_numpy()\n","    node2 = cofreq_graph['node2'].to_numpy()\n","    ppmi_vals = cofreq_graph['ppmi'].to_numpy()\n","\n","    for index in range(len(cofreq_graph.index)):\n","        id1 = node1[index]\n","        id2 = node2[index]\n","        ppmi = ppmi_vals[index]\n","\n","        if id1 in datasetDict:\n","            datasetDict[id1].append((id2, ppmi))\n","        else:\n","            datasetDict[id1] = [(id2, ppmi)]\n","        if id2 in datasetDict:\n","            datasetDict[id2].append((id1, ppmi))\n","        else:\n","            datasetDict[id2] = [(id1, ppmi)]\n","\n","    if (writeDatasets):\n","        print(\"Writing co-frequency dataset\")\n","        pickle.dump(datasetDict, open('../data/sym_data/sub_neighbors_dict_ppmi_perBin_1.pkl', \"wb\"), protocol=-1)\n","    else:\n","        print(\"Skipping write of co-frequency dataset\")\n","\n","\n","    # Store sample to visualize\n","    for term, adjList in datasetDict.items():\n","        examplePPMIRow = [term, adjList]\n","        break\n","    print(\"File written\")\n","\n","    # Split train test 90/10\n","    totalTermsShuffled = singleton_freqs.sample(frac=1)\n","    split = round(len(totalTermsShuffled.index) * 0.9)\n","    trainTerms = pd.DataFrame(totalTermsShuffled.to_numpy()[:split])\n","    trainTerms.columns = [\"node\", \"count\"]\n","    testTerms = pd.DataFrame(totalTermsShuffled.to_numpy()[split+1:])\n","    testTerms.columns = [\"node\", \"count\"]\n","\n","    # Use train/test terms to build synonyms data - the labels for the final model\n","    createSynonymGraphs(trainTerms, testTerms)\n","\n","\n","def subsampleTerms():\n","    '''\n","    # Subsample the terms list using approach from reference paper:\n","    #   Distributed Representations of Words and Phrases and their Compositionality\n","    '''\n","    \n","    singleton_freqs = pd.read_csv('../data/mappings/2_Singleton_Frequency_Counts/singlets_terms_perBin_1d.txt', delim_whitespace=True)\n","    singleton_freqs.columns = [\"node\", \"count\"]\n","    \n","    # Calculate term frequency\n","    singleton_freqs[\"freq\"] = singleton_freqs[\"count\"] / np.sum(singleton_freqs[\"count\"])\n","    \n","    print('Terms before subsampling: {0}'.format(len(singleton_freqs.index)))\n","    \n","    # Perform subsampling\n","    t = 10e-5\n","    singleton_freqs[\"prob\"] = 1 - np.sqrt( t / singleton_freqs[\"freq\"] )\n","    singleton_freqs[\"rand\"] = np.random.rand(len(singleton_freqs.index))\n","    singleton_freqs[\"remove\"] = singleton_freqs[\"prob\"] >= singleton_freqs[\"rand\"]\n","    singleton_freqs = singleton_freqs[singleton_freqs[\"remove\"] == False][[\"node\", \"count\"]]\n","\n","    print('Terms after subsampling: {0}'.format(len(singleton_freqs.index)))\n","\n","    return singleton_freqs\n","\n","\n","# ======================================\n","# Build synonym graphs using Term to Concept mapping\n","# ======================================\n","def createSynonymGraphs(train_terms, test_terms):\n","    global exampleSynonym\n","\n","    print(\"Building train/test synonym graphs\")\n","    synonyms = pd.read_csv('../data/mappings/3_ID_Mappings/3_term_ID_to_concept_ID.txt', delim_whitespace=True)\n","    synonyms.columns = [\"termID\", \"conceptID\"]\n","\n","    train_terms = train_terms.rename(columns={\"node\": \"termID\"})\n","    trainSyns = synonyms.merge(train_terms[\"termID\"].to_frame(), how='inner', on='termID')\n","\n","    test_terms = test_terms.rename(columns={\"node\": \"termID\"})\n","    testSyns = synonyms.merge(test_terms[\"termID\"].to_frame(), how='inner', on='termID')\n","\n","    # Build synonym edges for train\n","    termID = trainSyns['termID'].to_numpy()\n","    conceptID = trainSyns['conceptID'].to_numpy()\n","\n","    synonymDict = {}\n","    for index in range(len(trainSyns.index)-1):\n","        if (conceptID[index] in synonymDict):\n","            synonymDict[conceptID[index]].append(termID[index])\n","        else:\n","            synonymDict[conceptID[index]] = [termID[index]]\n","\n","    # Store sample to visualize\n","    synEdgesTrain = []\n","    for c, t in synonymDict.items():\n","        exampleSynonym = [c, t]\n","        break\n","    #\n","\n","    for concept, termList in synonymDict.items():\n","        for term in termList:\n","            for adjTerm in termList:\n","                if term != adjTerm:\n","                    synEdgesTrain.append([term, adjTerm])\n","\n","\n","    # Build synonym edges for test\n","    termID = testSyns['termID'].to_numpy()\n","    conceptID = testSyns['conceptID'].to_numpy()\n","\n","    synonymDict = {}\n","    for index in range(len(testSyns.index)-1):\n","        if (conceptID[index] in synonymDict):\n","            synonymDict[conceptID[index]].append(termID[index])\n","        else:\n","            synonymDict[conceptID[index]] = [termID[index]]\n","\n","    synEdgesTest = []\n","    for concept, termList in synonymDict.items():\n","        for term in termList:\n","            for adjTerm in termList:\n","                if term != adjTerm:\n","                    synEdgesTest.append([term, adjTerm])\n","    \n","\n","    trainDF = pd.DataFrame(synEdgesTrain, columns = ['term', 'adjterm'])\n","    testDF = pd.DataFrame(synEdgesTest, columns = ['term', 'adjterm'])\n","\n","    trainG = nx.from_pandas_edgelist(trainDF, source=\"term\", target=\"adjterm\")\n","    testG = nx.from_pandas_edgelist(testDF, source=\"term\", target=\"adjterm\")\n","    \n","    if (writeDatasets):\n","        print(\"Graphs are built, storing.\")\n","        pickle.dump(trainG, open(\"../data/sym_data/train_graph_nx_perBin_1.pkl\", 'wb'), protocol=-1)\n","        pickle.dump(testG, open(\"../data/sym_data/test_graph_nx_perBin_1.pkl\", 'wb'), protocol=-1)\n","        print(\"Graphs Stored\")\n","    else:\n","        print(\"Skipping storage of graphs.\")\n","\n","generateDatasetPkl()\n","\n"]},{"cell_type":"markdown","metadata":{"id":"mzcIN8-JW-1y"},"source":["## Examples of Processed Data\n","Below are example entries (mapped to the term/concept strings) of the generated datasets:\n","1. Subsampled PPMI Edges built from co-frequency graph\n","2. Synonym Labels (all terms under the same concept are deemed synonymous)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4028,"status":"ok","timestamp":1683382928770,"user":{"displayName":"Sathish Rama","userId":"12952249707319218415"},"user_tz":240},"id":"-2sCN8W1W-11","outputId":"5a18f8d6-24ed-46bf-9a0a-5398147027b4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Term: isosorbide mononitrate\n","Cofrequent Terms:\n","     Term: extended release tablet\n","          ->: 0.7239638154357573\n","     Term: extended release\n","          ->: 0.01802295225513008\n","     Term: apresoline\n","          ->: 0.5778422400713806\n","     Term: isosorbide\n","          ->: 1.5449863771375318\n","     Term: imdur\n","          ->: 1.1324074241194695\n","     Term: isosorbide mononitrate 30 mg\n","          ->: 2.5297275357897813\n","     Term: ranolazine\n","          ->: 0.8105997967439126\n","     Term: subclavian steal syndrome\n","          ->: 0.3386792427817149\n","     Term: isosorbide mononitrate 60 mg\n","          ->: 2.5297275357897813\n"]}],"source":["# Load TermID -> Term String mapping\n","termStringMapping = {}\n","with open('../data/mappings/3_ID_Mappings/1_term_ID_to_string.txt', \"r\", encoding = \"ISO-8859-1\") as file:\n","    for line in file:\n","        lineVals = line.split()\n","        termStringMapping[lineVals[0]] = ' '.join(lineVals[1:])\n","\n","\n","# Display Example of Term PPMIs\n","print('Term: {0}'.format(termStringMapping[str(examplePPMIRow[0])]))\n","print('Cofrequent Terms:')\n","for adj in examplePPMIRow[1]:\n","    print('     Term: {0}'.format(termStringMapping[str(adj[0])]))\n","    print('          ->: {0}'.format(adj[1]))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1241,"status":"ok","timestamp":1683382933811,"user":{"displayName":"Sathish Rama","userId":"12952249707319218415"},"user_tz":240},"id":"Kn10-8dGW-12","outputId":"9b87360c-15c1-4b44-9fb9-41efdcf880c7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Concept: diagnosis\n","Synonym Terms:\n","     diagnosis\n","     diagnosed\n"]}],"source":["# Load ConceptID -> String mapping\n","conceptStringMapping = {}\n","with open('../data/mappings/3_ID_Mappings/2a_concept_ID_to_string.txt', \"r\", encoding = \"ISO-8859-1\") as file:\n","    for line in file:\n","        lineVals = line.split()\n","        conceptStringMapping[lineVals[0]] = ' '.join(lineVals[1:])\n","\n","# Display Example of Concept Synonyms\n","print('Concept: {0}'.format(conceptStringMapping[str(exampleSynonym[0])]))\n","print('Synonym Terms:')\n","for syn in exampleSynonym[1]:\n","    print('     {0}'.format(termStringMapping[str(syn)]))"]},{"cell_type":"markdown","metadata":{"id":"GrF7NVbeW-13"},"source":["## Training Phase 1\n","The first portion of training is for Context Prediction. This training uses the pre-trained embeddings (word and subword) to generate a prediction of term's global context. For this problem statement, the co-frequency is a representation of a terms global context as it indicates its context of use among all other terms in the vocabulary. Using a predicted context rather than using the co-frequency graph allows for SurfCon to handle query terms that are not in the vocabulary. "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2792302,"status":"ok","timestamp":1683385733105,"user":{"displayName":"Sathish Rama","userId":"12952249707319218415"},"user_tz":240},"id":"GkCmG3zZW-14","outputId":"7a6c8c2b-365e-4a45-af57-035b369bdbdc"},"outputs":[{"name":"stdout","output_type":"stream","text":["args:  Namespace(embed_filename='../data/embeddings/glove.6B.100d.txt', ngram_embed_path='../data/embeddings/wiki-news-300d-1M-subword.vec', per='Bin', days='1', ngram_embed_dim=100, n_grams='2, 3, 4', node_embed_dim=128, word_hidden_dim=100, word_embed_dim=100, num_epochs=201, batch_size=10000, random_seed=43, dropout=0.5, log_interval=100, test_interval=1, early_stop_epochs=1000, learning_rate=0.001, save_best=True, save_dir='./saved_models/saved_pretrained/', save_interval=200, neg_sampling=False, num_negs=5)\n","Total number of candidates:  54060\n","Find 102937 grams with pretrain ratio: 0.37512264783314064\n","Find 84276 words with pretrain ratio: 0.7794745835113199\n","Model Parameters Stored!\n","ContextPredictionWordNGram(\n","  (ngrams_embeddings): Embedding(102938, 100)\n","  (w2v_embeddings): Embedding(84277, 100)\n","  (fc_out): Linear(in_features=200, out_features=128, bias=True)\n","  (context_out): Linear(in_features=128, out_features=54060, bias=False)\n","  (out): LogSoftmax(dim=1)\n",")\n","Training terms: 54060\n","0\n","54060\n","Begin trainning...\n","Epoch 0 - Train loss↓:10.89878882\n","05/06/2023 14:26:17\n","Epoch 1 - Train loss↓:10.86441237\n","Epoch 2 - Train loss↓:10.82823064\n","Epoch 3 - Train loss↓:10.78014041\n","Epoch 4 - Train loss↓:10.71394024\n","Epoch 5 - Train loss↓:10.62504249\n","Epoch 6 - Train loss↓:10.51363213\n","Epoch 7 - Train loss↓:10.38475435\n","Epoch 8 - Train loss↓:10.24319203\n","Epoch 9 - Train loss↓:10.09426794\n","Epoch 10 - Train loss↓:9.944246988\n","Epoch 11 - Train loss↓:9.79390203\n","Epoch 12 - Train loss↓:9.641171846\n","Epoch 13 - Train loss↓:9.488876509\n","Epoch 14 - Train loss↓:9.334678338\n","Epoch 15 - Train loss↓:9.177409794\n","Epoch 16 - Train loss↓:9.018953911\n","Epoch 17 - Train loss↓:8.862904932\n","Epoch 18 - Train loss↓:8.691168956\n","Epoch 19 - Train loss↓:8.525978947\n","Epoch 20 - Train loss↓:8.350112433\n","Epoch 21 - Train loss↓:8.185976953\n","Epoch 22 - Train loss↓:8.006103461\n","Epoch 23 - Train loss↓:7.832861346\n","Epoch 24 - Train loss↓:7.653745115\n","Epoch 25 - Train loss↓:7.470362647\n","Epoch 26 - Train loss↓:7.2879308\n","Epoch 27 - Train loss↓:7.105073934\n","Epoch 28 - Train loss↓:6.923144567\n","Epoch 29 - Train loss↓:6.744867826\n","Epoch 30 - Train loss↓:6.570644596\n","Epoch 31 - Train loss↓:6.380951865\n","Epoch 32 - Train loss↓:6.205378139\n","Epoch 33 - Train loss↓:6.025991086\n","Epoch 34 - Train loss↓:5.850467074\n","Epoch 35 - Train loss↓:5.680542019\n","Epoch 36 - Train loss↓:5.522273284\n","Epoch 37 - Train loss↓:5.358690648\n","Epoch 38 - Train loss↓:5.207987797\n","Epoch 39 - Train loss↓:5.051434243\n","Epoch 40 - Train loss↓:4.913407845\n","Epoch 41 - Train loss↓:4.796599843\n","Epoch 42 - Train loss↓:4.675727852\n","Epoch 43 - Train loss↓:4.539451897\n","Epoch 44 - Train loss↓:4.435555257\n","Epoch 45 - Train loss↓:4.341652576\n","Epoch 46 - Train loss↓:4.239142634\n","Epoch 47 - Train loss↓:4.149802158\n","Epoch 48 - Train loss↓:4.069414554\n","Epoch 49 - Train loss↓:3.980223869\n","Epoch 50 - Train loss↓:3.908101242\n","Epoch 51 - Train loss↓:3.838803239\n","Epoch 52 - Train loss↓:3.771591172\n","Epoch 53 - Train loss↓:3.711846863\n","Epoch 54 - Train loss↓:3.658244962\n","Epoch 55 - Train loss↓:3.602676424\n","Epoch 56 - Train loss↓:3.548586351\n","Epoch 57 - Train loss↓:3.497680529\n","Epoch 58 - Train loss↓:3.453745115\n","Epoch 59 - Train loss↓:3.422939572\n","Epoch 60 - Train loss↓:3.372358043\n","Epoch 61 - Train loss↓:3.333431748\n","Epoch 62 - Train loss↓:3.297513433\n","Epoch 63 - Train loss↓:3.261353957\n","Epoch 64 - Train loss↓:3.231628883\n","Epoch 65 - Train loss↓:3.183377338\n","Epoch 66 - Train loss↓:3.160275475\n","Epoch 67 - Train loss↓:3.125105713\n","Epoch 68 - Train loss↓:3.09668825\n","Epoch 69 - Train loss↓:3.068905535\n","Epoch 70 - Train loss↓:3.041231061\n","Epoch 71 - Train loss↓:3.01474436\n","Epoch 72 - Train loss↓:2.985700314\n","Epoch 73 - Train loss↓:2.961035012\n","Epoch 74 - Train loss↓:2.94375607\n","Epoch 75 - Train loss↓:2.917183996\n","Epoch 76 - Train loss↓:2.89302699\n","Epoch 77 - Train loss↓:2.874497701\n","Epoch 78 - Train loss↓:2.856505901\n","Epoch 79 - Train loss↓:2.834860969\n","Epoch 80 - Train loss↓:2.811587241\n","Epoch 81 - Train loss↓:2.786687101\n","Epoch 82 - Train loss↓:2.780112989\n","Epoch 83 - Train loss↓:2.757340441\n","Epoch 84 - Train loss↓:2.742978722\n","Epoch 85 - Train loss↓:2.716187454\n","Epoch 86 - Train loss↓:2.716677578\n","Epoch 87 - Train loss↓:2.681363553\n","Epoch 88 - Train loss↓:2.666133586\n","Epoch 89 - Train loss↓:2.646902133\n","Epoch 90 - Train loss↓:2.631143059\n","Epoch 91 - Train loss↓:2.618881401\n","Epoch 92 - Train loss↓:2.599620719\n","Epoch 93 - Train loss↓:2.591479483\n","Epoch 94 - Train loss↓:2.568824173\n","Epoch 95 - Train loss↓:2.558802466\n","Epoch 96 - Train loss↓:2.549258311\n","Epoch 97 - Train loss↓:2.535343939\n","Epoch 98 - Train loss↓:2.512051135\n","Epoch 99 - Train loss↓:2.499175576\n","Epoch 100 - Train loss↓:2.49989407\n","Epoch 101 - Train loss↓:2.478855019\n","Epoch 102 - Train loss↓:2.464540376\n","Epoch 103 - Train loss↓:2.450031974\n","Epoch 104 - Train loss↓:2.438631917\n","Epoch 105 - Train loss↓:2.428446908\n","Epoch 106 - Train loss↓:2.413163\n","Epoch 107 - Train loss↓:2.401093837\n","Epoch 108 - Train loss↓:2.390843977\n","Epoch 109 - Train loss↓:2.37832519\n","Epoch 110 - Train loss↓:2.364645656\n","Epoch 111 - Train loss↓:2.350235343\n","Epoch 112 - Train loss↓:2.352361814\n","Epoch 113 - Train loss↓:2.339594858\n","Epoch 114 - Train loss↓:2.324534986\n","Epoch 115 - Train loss↓:2.314224718\n","Epoch 116 - Train loss↓:2.301470805\n","Epoch 117 - Train loss↓:2.29213486\n","Epoch 118 - Train loss↓:2.276776274\n","Epoch 119 - Train loss↓:2.268911857\n","Epoch 120 - Train loss↓:2.260725857\n","Epoch 121 - Train loss↓:2.245059887\n","Epoch 122 - Train loss↓:2.243014597\n","Epoch 123 - Train loss↓:2.235562988\n","Epoch 124 - Train loss↓:2.223769885\n","Epoch 125 - Train loss↓:2.207770807\n","Epoch 126 - Train loss↓:2.20435031\n","Epoch 127 - Train loss↓:2.195156171\n","Epoch 128 - Train loss↓:2.181448094\n","Epoch 129 - Train loss↓:2.171197548\n","Epoch 130 - Train loss↓:2.16913069\n","Epoch 131 - Train loss↓:2.155593828\n","Epoch 132 - Train loss↓:2.151440493\n","Epoch 133 - Train loss↓:2.142138076\n","Epoch 134 - Train loss↓:2.133010558\n","Epoch 135 - Train loss↓:2.124557928\n","Epoch 136 - Train loss↓:2.113466735\n","Epoch 137 - Train loss↓:2.108758245\n","Epoch 138 - Train loss↓:2.10683489\n","Epoch 139 - Train loss↓:2.090051166\n","Epoch 140 - Train loss↓:2.085352032\n","Epoch 141 - Train loss↓:2.076104459\n","Epoch 142 - Train loss↓:2.06759254\n","Epoch 143 - Train loss↓:2.06181059\n","Epoch 144 - Train loss↓:2.054574344\n","Epoch 145 - Train loss↓:2.051536415\n","Epoch 146 - Train loss↓:2.043622248\n","Epoch 147 - Train loss↓:2.036034072\n","Epoch 148 - Train loss↓:2.027338006\n","Epoch 149 - Train loss↓:2.018807084\n","Epoch 150 - Train loss↓:2.01318074\n","Epoch 151 - Train loss↓:2.006067911\n","Epoch 152 - Train loss↓:1.999293609\n","Epoch 153 - Train loss↓:1.995791063\n","Epoch 154 - Train loss↓:1.990106768\n","Epoch 155 - Train loss↓:1.980731334\n","Epoch 156 - Train loss↓:1.973618144\n","Epoch 157 - Train loss↓:1.97212447\n","Epoch 158 - Train loss↓:1.969982499\n","Epoch 159 - Train loss↓:1.962559721\n","Epoch 160 - Train loss↓:1.957590272\n","Epoch 161 - Train loss↓:1.954062218\n","Epoch 162 - Train loss↓:1.946049528\n","Epoch 163 - Train loss↓:1.937395588\n","Epoch 164 - Train loss↓:1.938479417\n","Epoch 165 - Train loss↓:1.931978321\n","Epoch 166 - Train loss↓:1.928758628\n","Epoch 167 - Train loss↓:1.919015981\n","Epoch 168 - Train loss↓:1.911277798\n","Epoch 169 - Train loss↓:1.916006954\n","Epoch 170 - Train loss↓:1.902183446\n","Epoch 171 - Train loss↓:1.900840357\n","Epoch 172 - Train loss↓:1.892360376\n","Epoch 173 - Train loss↓:1.889250117\n","Epoch 174 - Train loss↓:1.882928112\n","Epoch 175 - Train loss↓:1.87949067\n","Epoch 176 - Train loss↓:1.882159688\n","Epoch 177 - Train loss↓:1.874339456\n","Epoch 178 - Train loss↓:1.871148268\n","Epoch 179 - Train loss↓:1.861824138\n","Epoch 180 - Train loss↓:1.857107662\n","Epoch 181 - Train loss↓:1.858433555\n","Epoch 182 - Train loss↓:1.849169253\n","Epoch 183 - Train loss↓:1.848771041\n","Epoch 184 - Train loss↓:1.84258676\n","Epoch 185 - Train loss↓:1.838139878\n","Epoch 186 - Train loss↓:1.833625832\n","Epoch 187 - Train loss↓:1.831797858\n","Epoch 188 - Train loss↓:1.828121676\n","Epoch 189 - Train loss↓:1.822579801\n","Epoch 190 - Train loss↓:1.821222224\n","Epoch 191 - Train loss↓:1.817949638\n","Epoch 192 - Train loss↓:1.812989184\n","Epoch 193 - Train loss↓:1.813742688\n","Epoch 194 - Train loss↓:1.806526349\n","Epoch 195 - Train loss↓:1.805819091\n","Epoch 196 - Train loss↓:1.798692064\n","Epoch 197 - Train loss↓:1.797858174\n","Epoch 198 - Train loss↓:1.790611777\n","Epoch 199 - Train loss↓:1.79282008\n","Epoch 200 - Train loss↓:1.790980616\n","05/06/2023 15:08:49\n"]}],"source":["#Uncomment to train context prediction model\n","# Requires \n","#!python -u main_pretrain.py --batch_size=10000 --learning_rate=0.001 --save_interval=200 --save_dir='./saved_models/saved_pretrained/' --ngram_embed_path='../data/embeddings/wiki-news-300d-1M-subword.vec'"]},{"cell_type":"markdown","metadata":{"id":"xMoWuvvlW-14"},"source":["## Training Phase 2\n","After training the context predictor model, it is time to train the ranking model. This training combines the inputs from a Query term and a Candidate term and attempts to predict whether the terms are synonyms. The combination of surface-form inputs is done directly, but the combination of the global contexts is done through a dynamic matching algorithm. This algorithm generates a semantic vectors for each term (query and candidate) and outputs a score based on their similarity."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1109424,"status":"ok","timestamp":1683387408026,"user":{"displayName":"Sathish Rama","userId":"12952249707319218415"},"user_tz":240},"id":"oyh8cX0RW-15","outputId":"a55e3834-252e-452f-89bf-181e3ee2be1c"},"outputs":[{"name":"stdout","output_type":"stream","text":["args:  Namespace(per='Bin', days='1', random_seed=42, num_oov=2000, re_sample_test=False, train_neg_num=50, test_neg_num=100, num_contexts=50, max_contexts=1000, context_gamma=0.3, ngram_embed_dim=100, n_grams='2, 3, 4', word_embed_dim=100, node_embed_dim=128, dropout=0, bi_out_dim=50, use_context=True, do_ctx_interact=True, num_epochs=3, log_interval=2000, test_interval=1, early_stop_epochs=10, metric='map', learning_rate=0.0001, min_epochs=2, clip_grad=5.0, lr_decay=0.05, embed_filename='../data/embeddings/glove.6B.100d.txt', node_embed_path='../data/embeddings/line2nd_ttcooc_embedding.txt', ngram_embed_path='../data/embeddings/wiki-news-300d-1M-subword.vec', restore_model_path='./saved_models/saved_pretrained/snapshot_epoch_200.pt', restore_idx_data='', logging=False, log_name='empty.txt', restore_model_epoch=600, save_best=True, save_dir='./saved_models', save_interval=5, random_test=True, neg_sampling=True, num_negs=5, rank_model_path=None)\n","********Key parameters:******\n","Use GPU? True\n","Model Parameters: \n","Dataset: Bin 1\n","Train # negative samples: 50\n","Test # negative samples: 100\n","# contexts to aggregate: 50\n","*****************************\n","./saved_models/rank_model_perBin_1\n","Begin loading data ...\n","Data: # train: 10434, # val: 148, # iv test: 149, # dis iv test 38\n","Data loaded!\n","(54060, 128)\n","-- Pre-stored parameters loaded! -- \n","Begin digitalizing ...\n","148 149 38\n","-- Word-Ngram Pretrained loaded! --\n","DeepTermRankingListNet(\n","  (dropout): Dropout(p=0, inplace=False)\n","  (TermEncoder): ContextPredictionWordNGram(\n","    (ngrams_embeddings): Embedding(102938, 100)\n","    (w2v_embeddings): Embedding(84277, 100)\n","    (fc_out): Linear(in_features=200, out_features=128, bias=True)\n","    (context_out): Linear(in_features=128, out_features=54060, bias=False)\n","    (out): LogSoftmax(dim=1)\n","    (context_embeddings): Embedding(54060, 128)\n","  )\n","  (ContextPredictor): ContextPredictionWordNGram(\n","    (ngrams_embeddings): Embedding(102938, 100)\n","    (w2v_embeddings): Embedding(84277, 100)\n","    (fc_out): Linear(in_features=200, out_features=128, bias=True)\n","    (context_out): Linear(in_features=128, out_features=54060, bias=False)\n","    (out): LogSoftmax(dim=1)\n","    (context_embeddings): Embedding(54060, 128)\n","  )\n","  (context_features): Embedding(54060, 128)\n","  (bi_c): Bilinear(in1_features=128, in2_features=128, out_features=1, bias=True)\n","  (out): LogSoftmax(dim=1)\n",")\n","['att_mat', 'TermEncoder.ngrams_embeddings.weight', 'TermEncoder.w2v_embeddings.weight', 'TermEncoder.fc_out.weight', 'TermEncoder.fc_out.bias', 'TermEncoder.context_out.weight', 'TermEncoder.context_embeddings.weight', 'ContextPredictor.ngrams_embeddings.weight', 'ContextPredictor.w2v_embeddings.weight', 'ContextPredictor.fc_out.weight', 'ContextPredictor.fc_out.bias', 'ContextPredictor.context_out.weight', 'ContextPredictor.context_embeddings.weight', 'context_features.weight', 'bi_c.weight', 'bi_c.bias']\n","Begin training...\n","Epoch-0, steps-2000: Train MAP - 0.75979, Train Loss - 3.9491\n","Epoch-0, steps-4000: Train MAP - 0.85019, Train Loss - 3.9415\n","Epoch-0, steps-6000: Train MAP - 0.88431, Train Loss - 3.9418\n","Epoch-0, steps-8000: Train MAP - 0.89417, Train Loss - 3.94\n","Epoch-0, steps-10000: Train MAP - 0.90008, Train Loss - 3.9398\n","Epoch-0: All Dev MAP: 0.82068\n","05/06/2023 15:24:32\n","--- Testing: All IV Test MAP: 0.77682\n","--- Testing: All Dis IV Test MAP: 0.54536\n","Epoch-1, steps-2000: Train MAP - 0.92482, Train Loss - 3.9381\n","Epoch-1, steps-4000: Train MAP - 0.94071, Train Loss - 3.9374\n","Epoch-1, steps-6000: Train MAP - 0.93465, Train Loss - 3.9364\n","Epoch-1, steps-8000: Train MAP - 0.94498, Train Loss - 3.9368\n","Epoch-1, steps-10000: Train MAP - 0.94296, Train Loss - 3.9365\n","Epoch-1: All Dev MAP: 0.86454\n","05/06/2023 15:30:35\n","--- Testing: All IV Test MAP: 0.79698\n","--- Testing: All Dis IV Test MAP: 0.5652\n","Epoch-2, steps-2000: Train MAP - 0.95621, Train Loss - 3.9358\n","Epoch-2, steps-4000: Train MAP - 0.96405, Train Loss - 3.9364\n","Epoch-2, steps-6000: Train MAP - 0.96789, Train Loss - 3.9344\n","Epoch-2, steps-8000: Train MAP - 0.96478, Train Loss - 3.9347\n","Epoch-2, steps-10000: Train MAP - 0.96415, Train Loss - 3.9347\n","Epoch-2: All Dev MAP: 0.90575\n","05/06/2023 15:36:38\n","--- Testing: All IV Test MAP: 0.84132\n","--- Testing: All Dis IV Test MAP: 0.59312\n"]}],"source":["#Uncomment to train ranking model\n","# Update --restore_model_path to the saved model epoch with best performance\n","# Required X RAM / GPU\n","#!python -u main_dym.py --use_context=True --restore_model_path='./saved_models/saved_pretrained/snapshot_epoch_200.pt' --min_epochs=2 --neg_sampling=True --num_contexts=50 --ngram_embed_path='../data/embeddings/wiki-news-300d-1M-subword.vec'"]},{"cell_type":"markdown","metadata":{"id":"dAYva0M5W-16"},"source":["## Testing\n","After both phases of training are completed, we can now utilize a simple interface with the model to take a `query` term in as input and predict the top 10 likely synonym terms."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zH2sXXrVW-16"},"outputs":[],"source":["# Uncomment to Query against model for synonyms\n","# Note when querying: results may be delayed ~20seconds\n","#!python -u main_testing.py --restore_model_path='./saved_models/saved_pretrained_fastext_nonzero/snapshot_epoch_5000.pt' --rank_model_path='./saved_models/rank_model_perBin_1/best_epoch_10.pt' --num_results=10 --cand_terms_path='' --use_context=True --ngram_embed_path='wiki-news-300d-1M-subword.vec' --neg_sampling=True"]},{"cell_type":"markdown","metadata":{"id":"bRv32tYFW-17"},"source":["## Results\n","\n","Below is the comparison of results between the work of original authors and our team. Note the below terms/shorthand:\n","- `InV`: Terms that are In Vocabulary. This means that the Query Term was a term present in the Co-Frequency graph dataset\n","- `Dissim`: A subset of synonymous terms that appear dissimilar. These indicate a harder subset of synonyms to identify due as they are visually quite different.\n","- `Context`: The `predicted context` portion of the model architecture (`Training Phase 1`) which attempts to generate a Query Term's top co-occuring terms.\n","\n","Interestingly, our results indicated slightly improved model performance without the `Context Predictions` portion of the model, unlike the original work. This improvement may not have been the case for OOV terms however, as we did not execute that portion of the experiment.  \n","\n","![Results Summary](../results/ResultsSummary.png \"Results Summary\")"]},{"cell_type":"markdown","metadata":{"id":"ubqAsZvqW-18"},"source":["## References\n","1. S. G. Finlayson, P. LePendu, and N. H. Shah. 2014. Building the graph of medicine from millions of clinical narratives. Scientific data 1 (2014), 140032.\n","2. H. J. Lowe, T. A. Ferris, P. M. Hernandez, and S. C. Weber. 2009. STRIDE–An integrated standards-based translational research informatics platform. In AMIA.\n","3. K. Hashimoto, Y. Tsuruoka, R. Socher, and o. 2017. A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks. In ACL.\n","4. K. Hashimoto, Y. Tsuruoka, R. Socher, and o. 2017. A Joint Many-Task Model:Growing a Neural Network for Multiple NLP Tasks. In ACL.\n","5. J. Wieting, M. Bansal, K. Gimpel, and K. Livescu. 2016. Charagram: Embedding words and sentences via character n-grams. In EMNLP.\n","6. P. Neculoiu, M. Versteegh, and M. Rotaru. 2016. Learning text similarity with siamese recurrent networks. In Workshop on Representation Learning for NLP.\n","7. M. Qu, X. Ren, and J. Han. 2017. Automatic synonym discovery with knowledge bases. In KDD.\n","8. SurfCon: Synonym Discovery on Privacy-Aware Clinical Data, https://dl.acm.org/doi/pdf/10.1145/3292500.3330894\n","9. W. Hamilton, Z. Ying, and J. Leskovec. 2017. Inductive representation learning on large graphs. In NeurIPS.\n","10. P. Velickovic, G. Cucurull, A. Casanova, A. Romero, P. Lio, and Y. Bengio. 2018. Graph attention networks. In ICLR.\n","11. Fastext embedding http://christopher5106.github.io/deep/learning/2020/04/02/fasttext_pretrained_embeddings_subword_word_representations.html\n","12. T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. 2013. Distributed representations of words and phrases and their compositionality. In NeurIPS\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[]},"gpuClass":"standard","interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"},"kernelspec":{"display_name":"Python 3.8.9 64-bit","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.9.6"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
